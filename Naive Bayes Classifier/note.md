# 朴素贝叶斯

## 贝叶斯原理

贝叶斯原理是怎么来的呢？贝叶斯为了解决一个叫“逆向概率”问题写了一篇文章，尝试解答在没有太多可靠证据的情况下，怎样做出更符合数学逻辑的推测。

什么是“逆向概率”呢？

所谓“逆向概率”是相对“正向概率”而言。正向概率的问题很容易理解，比如我们已经知道袋子里面有 N 个球，不是黑球就是白球，其中 M 个是黑球，那么把手伸进去摸一个球，就能知道摸出黑球的概率是多少。但这种情况往往是上帝视角，即了解了事情的全貌再做判断。

***先验概率：***

通过经验来判断事情发生的概率，比如说“贝叶死”的发病率是万分之一，就是先验概率。再比如南方的梅雨季是 6-7 月，就是通过往年的气候总结出来的经验，这个时候下雨的概率就比其他时间高出很多。

***后验概率：***

后验概率就是发生结果之后，推测原因的概率。比如说某人查出来了患有“贝叶死”，那么患病的原因可能是 A、B 或 C。患有“贝叶死”是因为原因 A 的概率就是后验概率。它是属于条件概率的一种。

***条件概率：***

事件 A 在另外一个事件 B 已经发生条件下的发生概率，表示为 P(A|B)，读作“在 B 发生的条件下 A 发生的概率”。比如原因 A 的条件下，患有“贝叶死”的概率，就是条件概率。

***似然函数（likelihood function）：***

你可以把概率模型的训练过程理解为求参数估计的过程。举个例子，如果一个硬币在 10 次抛落中正面均朝上。那么你肯定在想，这个硬币是均匀的可能性是多少？这里硬币均匀就是个参数，似然函数就是用来衡量这个模型的参数。似然在这里就是可能性的意思，它是关于统计参数的函数。

## 贝叶斯公式

$$P(B_i|A) = \frac{P(B_i)P(A|B_i)}{P(B_1)P(A|B_1)+P(B_2)P(A|B_2)}$$

***通用的贝叶斯公式：***

$$P(B_i|A) = \frac{P(B_i)P(A|B_i)}{\sum_{i=1}^nP(B_i)P(A|B_i)}$$

## 实例

**数据集：**

| 挂科 | 喝酒 | 逛街 | 学习 |
| ---- | ---- | ---- | ---- |
| 1 | 1 | 1 | 0 |
| 0 | 0 | 0 | 1 |
| 0 | 1 | 0 | 1 |
| 1 | 1 | 0 | 0 |
| 1 | 0 | 1 | 0 |
| 0 | 0 | 1 | 1 |
| 0 | 0 | 1 | 0 |
| 1 | 0 | 0 | 1 |

1：挂科、喝酒、逛街、学习
0：没挂科、没喝酒、没逛街、没学习

求没喝酒，没逛街，学习是挂科还是不挂科

$$P(挂科|没喝酒.没逛街.学习) = \frac{p(挂科)P(没喝酒.没逛街.学习|挂科)}{P(没喝酒.没逛街.学习)} = \frac{p(挂科)p(没喝酒|挂科)p(没逛街|挂科)p(学习|挂科)}{P(没喝酒)P(没逛街)P(学习)} = \frac{\frac{1}{2}*\frac{1}{2}*\frac{1}{2}*\frac{1}{4}}{\frac{5}{8}*\frac{1}{2}*\frac{1}{2}} = 0.4$$

$$P(没挂科|没喝酒.没逛街.学习) = \frac{p(没挂科)P(没喝酒.没逛街.学习|没挂科)}{P(没喝酒.没逛街.学习)} = \frac{p(没挂科)p(没喝酒|没挂科)p(没逛街|没挂科)p(学习|没挂科)}{P(没喝酒)P(没逛街)P(学习)} = \frac{\frac{1}{2}*\frac{3}{4}*\frac{1}{2}*\frac{3}{4}}{\frac{5}{8}*\frac{1}{2}*\frac{1}{2}} = 0.9$$

0.9 > 0.5，所以没喝酒，没逛街，学习的结果是没挂科的概率比较大。

## 贝叶斯算法

sklearn 的全称叫 Scikit-learn，它给我们提供了 3 个朴素贝叶斯分类算法，分别是高斯朴素贝叶斯（GaussianNB）、多项式朴素贝叶斯（MultinomialNB）和伯努利朴素贝叶斯（BernoulliNB）。

这三种算法适合应用在不同的场景下，我们应该根据特征变量的不同选择不同的算法：

- 高斯朴素贝叶斯：特征变量是连续变量，符合高斯分布，比如说人的身高，物体的长度。
- 多项式朴素贝叶斯：特征变量是离散变量，符合多项分布，在文档分类中特征变量体现在一个单词出现的次数，或者是单词的 TF-IDF 值等。
- 伯努利朴素贝叶斯：特征变量是布尔变量，符合 0/1 分布，在文档分类中特征是单词是否出现。

伯努利朴素贝叶斯是以文件为粒度，如果该单词在某文件中出现了即为 1，否则为 0。而多项式朴素贝叶斯是以单词为粒度，会计算在某个文件中的具体次数。而高斯朴素贝叶斯适合处理特征变量是连续变量，且符合正态分布（高斯分布）的情况。比如身高、体重这种自然界的现象就比较适合用高斯朴素贝叶斯来处理。而文本分类是使用多项式朴素贝叶斯或者伯努利朴素贝叶斯。

## TF-IDF 

TF-IDF 是一个统计方法，用来评估某个词语对于一个文件集或文档库中的其中一份文件的重要程度。

TF-IDF 实际上是两个词组 Term Frequency 和 Inverse Document Frequency 的总称，两者缩写为 TF 和 IDF，分别代表了词频和逆向文档频率。

**词频 TF** 计算了一个单词在文档中出现的次数，它认为一个单词的重要性和它在文档中出现的次数呈正比。

**逆向文档频率 IDF**，是指一个单词在文档中的区分度。它认为一个单词出现在的文档数越少，就越能通过这个单词把该文档和其他文档区分开。IDF 越大就代表该单词的区分度越大。

**所以 TF-IDF 实际上是词频 TF 和逆向文档频率 IDF 的乘积。**这样我们倾向于找到 TF 和 IDF 取值都高的单词作为区分，即这个单词在一个文档中出现的次数多，同时又很少出现在其他文档中。这样的单词适合用于分类。

***计算公式***

$$词频 TF = \frac{单词出现的次数}{该文档的总单词数}$$

$$逆向文档频率 IDF = log\frac{文档总数}{该单词出现的文档数+1}$$

$$TF-IDF=TF*IDF。$$